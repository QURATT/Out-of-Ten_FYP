{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb44467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "array_of_labeled_csv=['Bioderma_labeled.csv',\n",
    "                      'erave_labeled.csv',\n",
    "                      'neutrogena_labeled.csv',\n",
    "                      'labeled_mayb_foundation.csv',\n",
    "                      'labeled_mayb_conc.csv',\n",
    "                      'labeled_hp_envy.csv',\n",
    "                      'labeled_Macbook m1.csv',\n",
    "                      'labeled_lenovo x1.csv',\n",
    "                      'labeled_lenovo e14.csv',\n",
    "                      'galaxy7_labeled.csv'\n",
    "                     ]\n",
    "for product in range(len(array_of_labeled_csv)):\n",
    "    data = pd.read_csv(array_of_labeled_csv[product])\n",
    "    data.drop(['Positive','Negative','Neutral','Compound'],axis=1,inplace=True)\n",
    "    data.drop(data[(data['Sentiment'] =='Neutral')].index, inplace=True)\n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    def lemmatize_text(text):\n",
    "        st = \"\"\n",
    "        for w in w_tokenizer.tokenize(text):\n",
    "            st = st + lemmatizer.lemmatize(w) + \" \"\n",
    "        return st\n",
    "    data['Review'] = data.Review.apply(lemmatize_text)\n",
    "    reviews = data['Review'].values\n",
    "    labels = data['Sentiment'].values\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_labels = encoder.fit_transform(labels)\n",
    "    train_sentences, test_sentences, train_labels, test_labels = train_test_split(reviews, encoded_labels, stratify = encoded_labels)\n",
    "    vec = CountVectorizer(max_features = 3000)\n",
    "    X = vec.fit_transform(train_sentences)\n",
    "    vocab = vec.get_feature_names()\n",
    "    X = X.toarray()\n",
    "    word_counts = {}\n",
    "    for l in range(2):\n",
    "        word_counts[l] = defaultdict(lambda: 0)\n",
    "    for i in range(X.shape[0]):\n",
    "        l = train_labels[i]\n",
    "        for j in range(len(vocab)):\n",
    "            word_counts[l][vocab[j]] += X[i][j]\n",
    "    def laplace_smoothing(n_label_items, vocab, word_counts, word, text_label):\n",
    "        a = word_counts[text_label][word] + 1\n",
    "        b = n_label_items[text_label] + len(vocab)\n",
    "        return math.log(a/b)\n",
    "    def group_by_label(x, y, labels):\n",
    "        data = {}\n",
    "        for l in labels:\n",
    "            data[l] = x[np.where(y == l)]\n",
    "        return data\n",
    "    def fit(x, y, labels):\n",
    "        n_label_items = {}\n",
    "        log_label_priors = {}\n",
    "        n = len(x)\n",
    "        grouped_data = group_by_label(x, y, labels)\n",
    "        for l, data in grouped_data.items():\n",
    "            n_label_items[l] = len(data)\n",
    "            log_label_priors[l] = math.log(n_label_items[l] / n)\n",
    "        return n_label_items, log_label_priors\n",
    "    from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "    def predict(n_label_items, vocab, word_counts, log_label_priors, labels, x):\n",
    "        result = []\n",
    "        for text in x:\n",
    "            label_scores = {l: log_label_priors[l] for l in labels}\n",
    "            words = set(w_tokenizer.tokenize(text))\n",
    "            for word in words:\n",
    "                if word not in vocab: continue\n",
    "                for l in labels:\n",
    "                    log_w_given_l = laplace_smoothing(n_label_items, vocab, word_counts, word, l)\n",
    "                    label_scores[l] += log_w_given_l\n",
    "            result.append(max(label_scores, key=label_scores.get))\n",
    "        return result\n",
    "    labels = [0,1]\n",
    "    n_label_items, log_label_priors = fit(train_sentences,train_labels,labels)\n",
    "    pred = predict(n_label_items, vocab, word_counts, log_label_priors, labels, test_sentences)\n",
    "    #print(\"Prediction Results\")\n",
    "    #print(pred)\n",
    "    accuaracy=accuracy_score(test_labels,pred)*100\n",
    "    productname=array_of_labeled_csv[product]\n",
    "   # print(productname)\n",
    "    \n",
    "    #print(\"Accuracy : \", accuaracy,\"%\")\n",
    "    f = open(\"Accuracies.txt\", \"a\")\n",
    "    f.write(\"\\n\" + productname+ \" \" + str(accuaracy)+\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609b5a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-f3bc968c5475>:9: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(array_of_csv[product],sep='delimiter', header=None)\n",
      "<ipython-input-1-f3bc968c5475>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['Review'] = df['Review'].str.replace(r'[^\\w\\s]+', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral     1254\n",
      "Positive     532\n",
      "Negative     130\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-f3bc968c5475>:9: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(array_of_csv[product],sep='delimiter', header=None)\n",
      "<ipython-input-1-f3bc968c5475>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['Review'] = df['Review'].str.replace(r'[^\\w\\s]+', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral     3493\n",
      "Positive    2454\n",
      "Negative     743\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "array_of_csv=[\"lenovo x1.csv\",\"lenovo e14.csv\"\n",
    "                     ]\n",
    "for product in range(len(array_of_csv)):\n",
    "    df = pd.read_csv(array_of_csv[product],sep='delimiter', header=None)\n",
    "    df.columns =['Review']\n",
    "    df = df.dropna()\n",
    "    df['Review'] = df['Review'].str.replace(r'[^\\w\\s]+', '')\n",
    "    df.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n",
    "\n",
    "    sentiments = SentimentIntensityAnalyzer()\n",
    "    df[\"Positive\"] = [sentiments.polarity_scores(i)[\"pos\"] for i in df[\"Review\"]]\n",
    "    df[\"Negative\"] = [sentiments.polarity_scores(i)[\"neg\"] for i in df[\"Review\"]]\n",
    "    df[\"Neutral\"] = [sentiments.polarity_scores(i)[\"neu\"] for i in df[\"Review\"]]\n",
    "    df['Compound'] = [sentiments.polarity_scores(i)[\"compound\"] for i in df[\"Review\"]]\n",
    "    score = df[\"Compound\"].values\n",
    "    sentiment = []\n",
    "    for i in score:\n",
    "        if i >= 0.05 :\n",
    "            sentiment.append('Positive')\n",
    "        elif i <= -0.05 :\n",
    "            sentiment.append('Negative')\n",
    "        else:\n",
    "            sentiment.append('Neutral')\n",
    "    df[\"Sentiment\"] = sentiment\n",
    "    df.to_csv('labeled_'+ array_of_csv[product],index=False)\n",
    "    print(df[\"Sentiment\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dafd686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
